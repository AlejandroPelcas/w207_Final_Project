{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Historical ML to Predict the Fate of Accused Witches \n",
    "Elda Pere, Carlos Calderon, Alejandro Pelcastre, Kevin Gu \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we attempt to answer the question: **How well can we predict whether a witch trial will end in execution using gender and gender related features?**. The data comes from the University of Edinburgh's Historical Scottish Witchcraft Dataset, with information dated between the 15th and 18th century in Scotland. This topic ties into modern-day sexism. The purpose of it is to understand the roots of gender equality issues in history to be able to better understand their evolution into sexism today.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Might need to download some of these in your console. \n",
    "\n",
    "#conda install -c conda-forge imbalanced-learn\n",
    "#sudo pip install keras\n",
    "#pip install tensorflow\n",
    "#pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up complete\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import urllib.request as urllib2 # For python3\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib.colors import LogNorm\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.stats import norm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Set the randomizer seed so results are the same each time.\n",
    "np.random.seed(0)\n",
    "print(\"Set up complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccusedRef</th>\n",
       "      <th>AccusedSystemId</th>\n",
       "      <th>AccusedID</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>M_Firstname</th>\n",
       "      <th>M_Surname</th>\n",
       "      <th>Alias</th>\n",
       "      <th>Patronymic</th>\n",
       "      <th>DesTitle</th>\n",
       "      <th>...</th>\n",
       "      <th>Exec_county</th>\n",
       "      <th>Exec_burgh</th>\n",
       "      <th>Exec_NGR_Letters</th>\n",
       "      <th>Exec_NGR_Easting</th>\n",
       "      <th>Exec_NGR_Northing</th>\n",
       "      <th>PostTrialNotes</th>\n",
       "      <th>Createdby_y</th>\n",
       "      <th>Createdate_y</th>\n",
       "      <th>Lastupdatedby_y</th>\n",
       "      <th>Lastupdatedon_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A/EGD/10</td>\n",
       "      <td>EGD</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Mareon</td>\n",
       "      <td>Quheitt</td>\n",
       "      <td>Marion</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jhm</td>\n",
       "      <td>08/07/02 15:23:55</td>\n",
       "      <td>LEM</td>\n",
       "      <td>10/28/02 13:06:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A/EGD/100</td>\n",
       "      <td>EGD</td>\n",
       "      <td>100.0</td>\n",
       "      <td>Thom</td>\n",
       "      <td>Cockburn</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>Cockburn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LEM</td>\n",
       "      <td>07/24/02 16:01:21</td>\n",
       "      <td>LEM</td>\n",
       "      <td>07/24/02 16:01:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A/EGD/1000</td>\n",
       "      <td>EGD</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>Christian</td>\n",
       "      <td>Aitkenhead</td>\n",
       "      <td>Christine</td>\n",
       "      <td>Aikenhead</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LEM</td>\n",
       "      <td>07/18/01 16:13:27</td>\n",
       "      <td>jhm</td>\n",
       "      <td>10/01/02 10:48:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 205 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AccusedRef AccusedSystemId  AccusedID  FirstName    LastName M_Firstname  \\\n",
       "0    A/EGD/10             EGD       10.0     Mareon     Quheitt      Marion   \n",
       "1   A/EGD/100             EGD      100.0       Thom    Cockburn      Thomas   \n",
       "2  A/EGD/1000             EGD     1000.0  Christian  Aitkenhead   Christine   \n",
       "\n",
       "   M_Surname Alias Patronymic DesTitle  ... Exec_county  Exec_burgh  \\\n",
       "0      White   NaN        NaN      NaN  ...         NaN         NaN   \n",
       "1   Cockburn   NaN        NaN      NaN  ...         NaN         NaN   \n",
       "2  Aikenhead   NaN        NaN      NaN  ...         NaN         NaN   \n",
       "\n",
       "   Exec_NGR_Letters  Exec_NGR_Easting Exec_NGR_Northing PostTrialNotes  \\\n",
       "0               NaN               NaN               NaN            NaN   \n",
       "1               NaN               NaN               NaN            NaN   \n",
       "2               NaN               NaN               NaN            NaN   \n",
       "\n",
       "  Createdby_y       Createdate_y Lastupdatedby_y    Lastupdatedon_y  \n",
       "0         jhm  08/07/02 15:23:55             LEM  10/28/02 13:06:53  \n",
       "1         LEM  07/24/02 16:01:21             LEM  07/24/02 16:01:23  \n",
       "2         LEM  07/18/01 16:13:27             jhm  10/01/02 10:48:24  \n",
       "\n",
       "[3 rows x 205 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quick overview of data\n",
    "data = pd.read_csv(\"data/derived_accused_cases_trials.csv\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of our data: (3795, 205)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of our data:\",data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features we are interested in \n",
    "list_of_trial_features = ['Execution', 'Noreftocentral', 'Localwithcrep', 'Circuit', 'Cjtorder', 'Defence', 'High_status', \n",
    "                          'Male_accusers', 'Female_accusers', 'Confrontingsuspects', 'ActionDropped', 'Fled', 'Arrest', \n",
    "                          'Watching', 'TrialType']\n",
    "\n",
    "list_of_accused_features = ['AccusedRef', 'M_Firstname', 'M_Surname', 'Alias', 'Patronymic', 'DesTitle', 'Sex', 'Age', \n",
    "                            'Res_settlement', 'Res_parish', 'Res_presbytery',  'Res_county', 'Res_burgh', 'Ethnic_origin', \n",
    "                            'MaritalStatus', 'SocioecStatus', 'Occupation', 'Notes']\n",
    "\n",
    "list_of_cases_features = ['CaseRef', 'CaseStart_date', 'CaseStart_date_as_date', 'Case_date', 'Case_date_as_date', 'Age_at_case', \n",
    "                          'CaseCommonName', 'Complaint', 'Correspondence', 'Chronicle', 'Suspects_text', 'Familiars', 'Shape-Changing', \n",
    "                          'Dreams/Visions', 'UnorthodoxReligiousPractice', 'SympatheticMagic', 'Ridingdead', 'FolkNotes', \n",
    "                          'HumanIllness', 'HumanDeath', 'AnimalIllness', 'AnimalDeath', 'FemaleInfertility', 'MaleImpotence', \n",
    "                          'AggravatingDisease', 'TransferringDisease', 'LayingOn', 'Removalbewitchment', 'Quarreling', 'Cursing', \n",
    "                          'Poisoning', 'RecHealer', 'HealingHumans', 'HealingAnimals', 'Midwifery', 'DiseaseNotes', 'PropertyDamage', \n",
    "                          'WeatherModification', 'OtherMaleficiaNotes', 'OtherChargesNotes', 'ClaimedBewitched', 'ClaimedPossessed', \n",
    "                          'AdmitLesserCharge', 'ClaimedNaturalCauses', 'Nodefence', 'DefenseNotes', 'CaseNotes', 'PoliticalMotive_s', \n",
    "                          'PropertyMotive_p', 'PropertyMotive_s', 'RefusedCharity_p', 'RefusedCharity_s', 'Treason_p', 'Treason_s', \n",
    "                          'Other_p', 'Other_s', 'OtherText', 'NotEnoughInfo_p', 'NotEnoughInfo_s', 'WhiteMagic_p', 'WhiteMagic_s', \n",
    "                          'Charnotes', 'DemonicPact', 'DevilNotes', 'WitchesMeeting', 'MeetingName', 'DevilPresent', 'Maleficium', \n",
    "                          'CommunalSex', 'DevilWorship', 'FoodAndDrink', 'Dancing', 'Singing', 'SingingText', 'OtherPractices', \n",
    "                          'MeetingNotes', 'Elphane/Fairyland', 'Food/Drink', 'SpecificVerbalFormulae', 'SpecificRitualActs', \n",
    "                          'UNorthodoxRelPract_p', 'UNorthodoxRelPract_s', 'Consulting_p', 'Consulting_s', 'Demonic_p', 'Demonic_s', \n",
    "                          'Demonic_possess_p', 'Demonic_possess_s', 'Fairies_p', 'Fairies_s', 'Folk_healing_p', 'Folk_healing_s', \n",
    "                          'Maleficium_p', 'Maleficium_s', 'Midwifery_p', 'Midwifery_s', 'ImplicatedByAnother_p', 'ImplicatedByAnother_s', \n",
    "                          'Neighbhd_dispute_p', 'Neighbhd_dispute_s', 'PoliticalMotive_p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limit data to important features.\n",
    "all_features = list_of_accused_features + list_of_cases_features + list_of_trial_features\n",
    "df = data[all_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns with more than 70% missing values. This helps us weed out less important features\n",
    "percent_non_null = 0.7\n",
    "df = df.dropna(axis = 1, thresh=df.shape[0]*percent_non_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AccusedRef</th>\n",
       "      <th>M_Firstname</th>\n",
       "      <th>M_Surname</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Res_presbytery</th>\n",
       "      <th>Res_county</th>\n",
       "      <th>CaseRef</th>\n",
       "      <th>CaseStart_date</th>\n",
       "      <th>CaseStart_date_as_date</th>\n",
       "      <th>Case_date</th>\n",
       "      <th>...</th>\n",
       "      <th>Defence</th>\n",
       "      <th>High_status</th>\n",
       "      <th>Male_accusers</th>\n",
       "      <th>Female_accusers</th>\n",
       "      <th>Confrontingsuspects</th>\n",
       "      <th>ActionDropped</th>\n",
       "      <th>Fled</th>\n",
       "      <th>Arrest</th>\n",
       "      <th>Watching</th>\n",
       "      <th>TrialType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3594</td>\n",
       "      <td>3550</td>\n",
       "      <td>3558</td>\n",
       "      <td>3544</td>\n",
       "      <td>2799</td>\n",
       "      <td>3474</td>\n",
       "      <td>3788</td>\n",
       "      <td>2971</td>\n",
       "      <td>2971</td>\n",
       "      <td>3774</td>\n",
       "      <td>...</td>\n",
       "      <td>3211.000000</td>\n",
       "      <td>3211.000000</td>\n",
       "      <td>3209.000000</td>\n",
       "      <td>3208.000000</td>\n",
       "      <td>3211.000000</td>\n",
       "      <td>3211.000000</td>\n",
       "      <td>3211.00000</td>\n",
       "      <td>3211.000000</td>\n",
       "      <td>3211.000000</td>\n",
       "      <td>3211.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3219</td>\n",
       "      <td>159</td>\n",
       "      <td>1168</td>\n",
       "      <td>2</td>\n",
       "      <td>74</td>\n",
       "      <td>34</td>\n",
       "      <td>3413</td>\n",
       "      <td>956</td>\n",
       "      <td>937</td>\n",
       "      <td>1164</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>A/EGD/1343</td>\n",
       "      <td>Janet</td>\n",
       "      <td>Thomson</td>\n",
       "      <td>Female</td>\n",
       "      <td>Haddington</td>\n",
       "      <td>Haddington</td>\n",
       "      <td>C/EGD/831</td>\n",
       "      <td>1649</td>\n",
       "      <td>06/01/49 00:00:00</td>\n",
       "      <td>17/4/1662</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>544</td>\n",
       "      <td>66</td>\n",
       "      <td>3030</td>\n",
       "      <td>459</td>\n",
       "      <td>658</td>\n",
       "      <td>3</td>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>93</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.008409</td>\n",
       "      <td>0.418199</td>\n",
       "      <td>0.255923</td>\n",
       "      <td>0.032077</td>\n",
       "      <td>0.012457</td>\n",
       "      <td>0.03052</td>\n",
       "      <td>0.151355</td>\n",
       "      <td>0.006851</td>\n",
       "      <td>2.536593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.113635</td>\n",
       "      <td>0.091326</td>\n",
       "      <td>2.510500</td>\n",
       "      <td>1.627914</td>\n",
       "      <td>0.176233</td>\n",
       "      <td>0.110932</td>\n",
       "      <td>0.17204</td>\n",
       "      <td>0.358450</td>\n",
       "      <td>0.082502</td>\n",
       "      <td>0.927673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        AccusedRef M_Firstname M_Surname     Sex Res_presbytery  Res_county  \\\n",
       "count         3594        3550      3558    3544           2799        3474   \n",
       "unique        3219         159      1168       2             74          34   \n",
       "top     A/EGD/1343       Janet   Thomson  Female     Haddington  Haddington   \n",
       "freq             3         544        66    3030            459         658   \n",
       "mean           NaN         NaN       NaN     NaN            NaN         NaN   \n",
       "std            NaN         NaN       NaN     NaN            NaN         NaN   \n",
       "min            NaN         NaN       NaN     NaN            NaN         NaN   \n",
       "25%            NaN         NaN       NaN     NaN            NaN         NaN   \n",
       "50%            NaN         NaN       NaN     NaN            NaN         NaN   \n",
       "75%            NaN         NaN       NaN     NaN            NaN         NaN   \n",
       "max            NaN         NaN       NaN     NaN            NaN         NaN   \n",
       "\n",
       "          CaseRef CaseStart_date CaseStart_date_as_date  Case_date  ...  \\\n",
       "count        3788           2971                   2971       3774  ...   \n",
       "unique       3413            956                    937       1164  ...   \n",
       "top     C/EGD/831           1649      06/01/49 00:00:00  17/4/1662  ...   \n",
       "freq            3             62                     62         93  ...   \n",
       "mean          NaN            NaN                    NaN        NaN  ...   \n",
       "std           NaN            NaN                    NaN        NaN  ...   \n",
       "min           NaN            NaN                    NaN        NaN  ...   \n",
       "25%           NaN            NaN                    NaN        NaN  ...   \n",
       "50%           NaN            NaN                    NaN        NaN  ...   \n",
       "75%           NaN            NaN                    NaN        NaN  ...   \n",
       "max           NaN            NaN                    NaN        NaN  ...   \n",
       "\n",
       "            Defence  High_status  Male_accusers  Female_accusers  \\\n",
       "count   3211.000000  3211.000000    3209.000000      3208.000000   \n",
       "unique          NaN          NaN            NaN              NaN   \n",
       "top             NaN          NaN            NaN              NaN   \n",
       "freq            NaN          NaN            NaN              NaN   \n",
       "mean       0.013080     0.008409       0.418199         0.255923   \n",
       "std        0.113635     0.091326       2.510500         1.627914   \n",
       "min        0.000000     0.000000       0.000000         0.000000   \n",
       "25%        0.000000     0.000000       0.000000         0.000000   \n",
       "50%        0.000000     0.000000       0.000000         0.000000   \n",
       "75%        0.000000     0.000000       0.000000         0.000000   \n",
       "max        1.000000     1.000000      48.000000        27.000000   \n",
       "\n",
       "        Confrontingsuspects  ActionDropped        Fled       Arrest  \\\n",
       "count           3211.000000    3211.000000  3211.00000  3211.000000   \n",
       "unique                  NaN            NaN         NaN          NaN   \n",
       "top                     NaN            NaN         NaN          NaN   \n",
       "freq                    NaN            NaN         NaN          NaN   \n",
       "mean               0.032077       0.012457     0.03052     0.151355   \n",
       "std                0.176233       0.110932     0.17204     0.358450   \n",
       "min                0.000000       0.000000     0.00000     0.000000   \n",
       "25%                0.000000       0.000000     0.00000     0.000000   \n",
       "50%                0.000000       0.000000     0.00000     0.000000   \n",
       "75%                0.000000       0.000000     0.00000     0.000000   \n",
       "max                1.000000       1.000000     1.00000     1.000000   \n",
       "\n",
       "           Watching    TrialType  \n",
       "count   3211.000000  3211.000000  \n",
       "unique          NaN          NaN  \n",
       "top             NaN          NaN  \n",
       "freq            NaN          NaN  \n",
       "mean       0.006851     2.536593  \n",
       "std        0.082502     0.927673  \n",
       "min        0.000000     1.000000  \n",
       "25%        0.000000     2.000000  \n",
       "50%        0.000000     2.000000  \n",
       "75%        0.000000     4.000000  \n",
       "max        1.000000     4.000000  \n",
       "\n",
       "[11 rows x 104 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                 'percent_missing': percent_missing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Res_presbytery</th>\n",
       "      <td>Res_presbytery</td>\n",
       "      <td>26.245059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CaseStart_date</th>\n",
       "      <td>CaseStart_date</td>\n",
       "      <td>21.712780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CaseStart_date_as_date</th>\n",
       "      <td>CaseStart_date_as_date</td>\n",
       "      <td>21.712780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Female_accusers</th>\n",
       "      <td>Female_accusers</td>\n",
       "      <td>15.467721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Male_accusers</th>\n",
       "      <td>Male_accusers</td>\n",
       "      <td>15.441370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   column_name  percent_missing\n",
       "Res_presbytery                  Res_presbytery        26.245059\n",
       "CaseStart_date                  CaseStart_date        21.712780\n",
       "CaseStart_date_as_date  CaseStart_date_as_date        21.712780\n",
       "Female_accusers                Female_accusers        15.467721\n",
       "Male_accusers                    Male_accusers        15.441370"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_value_df.sort_values(\"percent_missing\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_missing = df.isna().sum() * 100 / len(df)\n",
    "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                 'percent_missing': percent_missing})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NotEnoughInfo_p</th>\n",
       "      <td>NotEnoughInfo_p</td>\n",
       "      <td>0.184453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DevilPresent</th>\n",
       "      <td>DevilPresent</td>\n",
       "      <td>0.184453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WitchesMeeting</th>\n",
       "      <td>WitchesMeeting</td>\n",
       "      <td>0.184453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DemonicPact</th>\n",
       "      <td>DemonicPact</td>\n",
       "      <td>0.184453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WhiteMagic_s</th>\n",
       "      <td>WhiteMagic_s</td>\n",
       "      <td>0.184453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     column_name  percent_missing\n",
       "NotEnoughInfo_p  NotEnoughInfo_p         0.184453\n",
       "DevilPresent        DevilPresent         0.184453\n",
       "WitchesMeeting    WitchesMeeting         0.184453\n",
       "DemonicPact          DemonicPact         0.184453\n",
       "WhiteMagic_s        WhiteMagic_s         0.184453"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_value_df.sort_values(\"percent_missing\", ascending=True).head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within our usable columns, let's choose some features that can be useful in determining execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features = ['Execution', 'Arrest', 'Fled', 'ActionDropped', 'High_status', 'Male_accusers', 'Female_accusers', \n",
    "                      'WhiteMagic_p', 'WhiteMagic_s', 'Folk_healing_p', 'Folk_healing_s', 'Demonic_possess_p', 'Demonic_possess_s', \n",
    "                     'UNorthodoxRelPract_p', 'UNorthodoxRelPract_s', 'UnorthodoxReligiousPractice', 'HealingHumans', 'HealingAnimals', \n",
    "                     'Neighbhd_dispute_p', 'Neighbhd_dispute_s', 'PoliticalMotive_p', 'PoliticalMotive_s', 'PropertyMotive_p', 'PropertyMotive_s', \n",
    "                     'RefusedCharity_p', 'RefusedCharity_s', 'Treason_p', 'Treason_s', 'Quarreling', 'Cursing', 'Poisoning', 'RecHealer', \n",
    "                     'Dancing', 'Singing', 'Midwifery']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some of the distributions of these features, including Execution itself. We'll first exclude the ones that aren't binary. When the value of a feature is 1, that means that feature was a reason behind someone's accusation. For example, for Quarreling, a 1 means that the person quarreling (with strangers, neighbors, etc.) was one reason behind why they were accused of being a witch. A 0 means it wasn't a reason. Some variables have a p or s attached, in which case p means it was a primary reason for accusation, and s secondary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Execution</th>\n",
       "      <td>2981</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arrest</th>\n",
       "      <td>2725</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fled</th>\n",
       "      <td>3113</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ActionDropped</th>\n",
       "      <td>3171</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High_status</th>\n",
       "      <td>3184</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WhiteMagic_p</th>\n",
       "      <td>3781</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WhiteMagic_s</th>\n",
       "      <td>3695</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Folk_healing_p</th>\n",
       "      <td>3740</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Folk_healing_s</th>\n",
       "      <td>3618</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demonic_possess_p</th>\n",
       "      <td>3730</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demonic_possess_s</th>\n",
       "      <td>3701</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNorthodoxRelPract_p</th>\n",
       "      <td>3788</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UNorthodoxRelPract_s</th>\n",
       "      <td>3677</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UnorthodoxReligiousPractice</th>\n",
       "      <td>3677</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HealingHumans</th>\n",
       "      <td>3646</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HealingAnimals</th>\n",
       "      <td>3726</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neighbhd_dispute_p</th>\n",
       "      <td>3723</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neighbhd_dispute_s</th>\n",
       "      <td>3633</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoliticalMotive_p</th>\n",
       "      <td>3785</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoliticalMotive_s</th>\n",
       "      <td>3718</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PropertyMotive_p</th>\n",
       "      <td>3782</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PropertyMotive_s</th>\n",
       "      <td>3710</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RefusedCharity_p</th>\n",
       "      <td>3778</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RefusedCharity_s</th>\n",
       "      <td>3765</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Treason_p</th>\n",
       "      <td>3782</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Treason_s</th>\n",
       "      <td>3757</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quarreling</th>\n",
       "      <td>3616</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cursing</th>\n",
       "      <td>3623</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisoning</th>\n",
       "      <td>3765</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RecHealer</th>\n",
       "      <td>3733</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dancing</th>\n",
       "      <td>3633</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Singing</th>\n",
       "      <td>3764</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Midwifery</th>\n",
       "      <td>3772</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0    1\n",
       "Execution                    2981  230\n",
       "Arrest                       2725  486\n",
       "Fled                         3113   98\n",
       "ActionDropped                3171   40\n",
       "High_status                  3184   27\n",
       "WhiteMagic_p                 3781    7\n",
       "WhiteMagic_s                 3695   93\n",
       "Folk_healing_p               3740   48\n",
       "Folk_healing_s               3618  170\n",
       "Demonic_possess_p            3730   58\n",
       "Demonic_possess_s            3701   87\n",
       "UNorthodoxRelPract_p         3788    0\n",
       "UNorthodoxRelPract_s         3677  111\n",
       "UnorthodoxReligiousPractice  3677  111\n",
       "HealingHumans                3646  142\n",
       "HealingAnimals               3726   62\n",
       "Neighbhd_dispute_p           3723   65\n",
       "Neighbhd_dispute_s           3633  155\n",
       "PoliticalMotive_p            3785    3\n",
       "PoliticalMotive_s            3718   70\n",
       "PropertyMotive_p             3782    6\n",
       "PropertyMotive_s             3710   78\n",
       "RefusedCharity_p             3778   10\n",
       "RefusedCharity_s             3765   23\n",
       "Treason_p                    3782    6\n",
       "Treason_s                    3757   31\n",
       "Quarreling                   3616  172\n",
       "Cursing                      3623  165\n",
       "Poisoning                    3765   23\n",
       "RecHealer                    3733   55\n",
       "Dancing                      3633  155\n",
       "Singing                      3764   24\n",
       "Midwifery                    3772   16"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_0s = []\n",
    "num_1s = []\n",
    "for feature in important_features:\n",
    "    value_counts = data[feature].value_counts()\n",
    "    if len(value_counts) == 2:\n",
    "        num_0s.append(value_counts[0])\n",
    "        num_1s.append(value_counts[1])\n",
    "    # And just in case any column would happen to have all 0's\n",
    "    if len(value_counts) == 1:\n",
    "        num_0s.append(value_counts[0])\n",
    "        num_1s.append(0)\n",
    "        #print(value_counts)\n",
    "        #print()\n",
    "\n",
    "pd.DataFrame(list(zip(num_0s, num_1s)), index = [i for i in important_features if len(data[i].value_counts()) <= 2], \n",
    "                                                 columns = [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the two features that aren't binary, the number of male and female accusers the accused had. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4184fe1dd8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.Male_accusers.plot.hist(bins = 30, xlim = (0,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4184f86048>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.Female_accusers.plot.hist(bins = 30, xlim = (0,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_important_features = df[important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the sex column, but changed to 0-1 format\n",
    "df_sex = pd.get_dummies(df.Sex)\n",
    "df_important_features = pd.concat((df_sex, df_important_features), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_important_features = df_important_features.drop(['Male'], axis = 1)\n",
    "df_important_features = df_important_features.rename(columns={\"Female\": \"Sex\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_features.insert(1, \"Sex\")\n",
    "df_important_features = df_important_features.reindex(columns = important_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Female --> 1\")\n",
    "print(\"Male --> 0\")\n",
    "df_important_features.Sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(feature):\n",
    "    \"\"\" quick function to print out summaries of features of interest\"\"\"\n",
    "    #Print out all feature values\n",
    "    #print(\"The rows of the feature are,\" , str(feature))\n",
    "    \n",
    "    #print value counts\n",
    "    print('' )\n",
    "    print(\"The values are, \",feature.value_counts() )\n",
    "    print('')\n",
    "    \n",
    "    #print most seen value\n",
    "    mode = max(feature.value_counts())\n",
    "    print(\"The most seen value is,\", mode )\n",
    "    print('')\n",
    "    \n",
    "    #Get unique values of feature\n",
    "    output = []\n",
    "    for x in feature.value_counts():\n",
    "        if x not in output:\n",
    "            output.append(x)\n",
    "    print(\"The unique value counts are\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_important_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ad862260cdfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_important_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_important_features' is not defined"
     ]
    }
   ],
   "source": [
    "summary(df_important_features.Execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Null Values\n",
    "Deleted data columns that were not of type float. Then filled in NaN values with a random value from the column with probability proportional to the frequency of value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_important_features['Sex']=df_important_features['Sex'].astype('float64')\n",
    "integer_columns = df_important_features.select_dtypes(include=['int64']).columns #this list is empty but here to show I looked for integer types\n",
    "float_columns = df_important_features.select_dtypes(include=['float64']).columns\n",
    "\n",
    "#get columns with float dtypes\n",
    "df_important_features = df_important_features[float_columns]\n",
    "\n",
    "#For each column, sum all the values vertically and skip the null values\n",
    "#Show the column and how many Null values\n",
    "print('Total null values before transformation: ' + str(df_important_features.sum(axis = 0, skipna = True).sum()))\n",
    "#sum up the values (to be used if we wanna incorporate filling with mean but I don't do it in this cell)\n",
    "sum_series = df_important_features.sum(axis = 0, skipna = True)\n",
    "columns = sum_series.index\n",
    "    \n",
    "for col in df_important_features:\n",
    "    values = []\n",
    "    for value in df_important_features[col]:\n",
    "        if not np.isnan(value):\n",
    "            values.append(value)\n",
    "    if len(values) > 0:\n",
    "        while df_important_features[col].isna().sum() > 0:\n",
    "            df_important_features[col].fillna(random.choice(values), inplace = True, limit = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total null values left in the data: ' + str(df_important_features[columns].isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_important_features.drop('Execution', 1)\n",
    "Y = df_important_features.Execution\n",
    "\n",
    "train_test_split = int(0.8 * X.shape[0])\n",
    "train_dev_split = (int)(2/3 * train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = X[:train_test_split], Y[:train_test_split]\n",
    "test_data, test_labels = X[train_test_split:], Y[train_test_split:]\n",
    "\n",
    "print(\"training data shape is:\",train_data.shape, \"Test data shape is:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying SMOTE for Imbalanced Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Training Data: \n",
    "print(\"Rebalancing training data:\")\n",
    "# summarize class distribution\n",
    "print(\"Classes before SMOTE: \")\n",
    "counter = Counter(train_labels)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample_train = SMOTE(random_state=0)\n",
    "train_data, train_labels = oversample_train.fit_resample(train_data, train_labels)\n",
    "# summarize the new class distribution\n",
    "print(\"Classes after SMOTE: \")\n",
    "counter = Counter(train_labels)\n",
    "print(counter)\n",
    "print(\"-\"*30)\n",
    "\n",
    "#For Dev Data: \n",
    "print(\"Rebalancing development data:\")\n",
    "# summarize class distribution\n",
    "print(\"Classes before SMOTE: \")\n",
    "counter = Counter(dev_labels)\n",
    "print(counter)\n",
    "# transform the dataset\n",
    "oversample_dev = SMOTE(random_state=0)\n",
    "dev_data, dev_labels = oversample_dev.fit_resample(dev_data, dev_labels)\n",
    "# summarize the new class distribution\n",
    "print(\"Classes after SMOTE: \")\n",
    "counter = Counter(dev_labels)\n",
    "print(counter)\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis, Fraction of Total Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 35 features selected, it makes sense to do some dimensionality reduction using Principal Component Analysis. Let's see how well different numbers of principal components can still explain our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = train_data.shape[1])\n",
    "X_train_pca = pca.fit_transform(train_data.fillna(0))\n",
    "explained_variances = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "display(pd.DataFrame(explained_variances[:5], index={\"n = 1\",\"n = 2\",'n = 3','n = 4','n = 5'}))\n",
    "\n",
    "plt.plot(explained_variances);\n",
    "plt.xlabel(\"Number of principal components\");\n",
    "plt.ylabel(\"Proportion of total variance explained\");\n",
    "plt.title(\"Total variance explained vs number of PC's\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what 2 principal components can capture in terms of the executed versus not executed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(train_data)\n",
    "X_train_pca_df = pd.DataFrame(X_train_pca)\n",
    "X_train_pca_df = X_train_pca_df.assign(label = train_labels)\n",
    "X_train_pca_df_non_executed, X_train_pca_df_executed = X_train_pca_df[X_train_pca_df['label'] == 0], X_train_pca_df[X_train_pca_df['label'] == 1]\n",
    "plt.scatter(X_train_pca_df_non_executed.loc[:, 0], X_train_pca_df_non_executed.loc[:, 1], color = 'blue', label = \"non-executed\")\n",
    "plt.scatter(X_train_pca_df_executed.loc[:, 0], X_train_pca_df_executed.loc[:, 1], color = 'red', label = \"executed\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"1st Principal Component\")\n",
    "plt.ylabel(\"2nd Principal Component\")\n",
    "plt.title(\"First two PC's, distinguished by executed vs non-executed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save some dataframes with more Principal Components of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of additional PC's\n",
    "pca_train_dataframes = []\n",
    "pca_test_dataframes = []\n",
    "for i in range(2, 7):\n",
    "    pca_model = PCA(n_components=i)\n",
    "    \n",
    "    # Feature selection for training data\n",
    "    pca_data = pca_model.fit_transform(train_data)\n",
    "    pca_train_dataframes.append(pd.DataFrame(pca_data))\n",
    "    \n",
    "    # Feature selection for test data \n",
    "    # Currently replacing all NaN values with 0. Might change this \n",
    "    # based on what Alejandro figures out \n",
    "    pca_test = pca_model.transform(test_data)\n",
    "    pca_test_dataframes.append(pd.DataFrame(pca_test))\n",
    "\n",
    "pca_train_dataframes[3].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model for this project will be a logistic regression model since we are working on a binary classification problem. After applying the transformations above, we experimented with different parameters such as using L1 regularization instead of the default L2. Although this would have reduce some features, the performance metrics did not improve (accuracy = 0.826) and so we kept the default value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(random_state = 0)\n",
    "\n",
    "#Fit model then print the features with their coefficient values from the logistic regression model \n",
    "lr_model.fit(train_data, train_labels)\n",
    "features_coef = []\n",
    "for i in range(0, 35):\n",
    "    features_coef.append((lr_model.coef_[0][i], train_data.columns[i]))\n",
    "\n",
    "#score data\n",
    "print(\"The model accuracy for the test data is: \" + str(lr_model.score(test_data, test_labels)))\n",
    "#Turn to data frame\n",
    "coefs = pd.DataFrame(features_coef, columns=['Coefficient_value', 'Feature_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we check p-values to see if we can reject the null hypothesis that the variables have no real effect on the 'Execution' variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referenced from stackoverflow.com/questions/25122999/scikit-learn-how-to-check-coefficients-significance  \n",
    "def logit_pvalue(model, x):\n",
    "    \"\"\" Calculate z-scores for scikit-learn LogisticRegression.\n",
    "    parameters:\n",
    "        model: fitted sklearn.linear_model.LogisticRegression with intercept and large C\n",
    "        x:     matrix on which the model was fit\n",
    "    This function uses asymtptics for maximum likelihood estimates.\n",
    "    \"\"\"\n",
    "    #Adding noise to avoid perfect collinearity\n",
    "    x = x+0.00001*np.random.rand(len(x), len(x.columns))\n",
    "    p = model.predict_proba(x)\n",
    "    n = len(p)\n",
    "    m = len(model.coef_[0]) + 1\n",
    "    coefs = np.concatenate([model.intercept_, model.coef_[0]])\n",
    "    x_full = np.matrix(np.insert(np.array(x), 0, 1, axis = 1))\n",
    "    ans = np.zeros((m, m))\n",
    "    for i in range(n):\n",
    "        ans = ans + np.dot(np.transpose(x_full[i, :]), x_full[i, :]) * p[i,1] * p[i, 0]\n",
    "    vcov = np.linalg.inv(np.matrix(ans))\n",
    "    se = np.sqrt(np.diag(vcov))\n",
    "    t =  coefs/se  \n",
    "    p = (1 - norm.cdf(abs(t))) * 2\n",
    "    return p\n",
    "\n",
    "# test p-values\n",
    "coefs['P Value'] = pd.DataFrame(logit_pvalue(lr_model, train_data))\n",
    "coefs['Is Significant'] = coefs['P Value'] < 0.05\n",
    "coefs['Odds Ratio'] = np.exp(coefs['Coefficient_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = coefs.sort_values(by = 'Coefficient_value', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The coefficients with the maximum distance away from 0 are the more influential features. \n",
    "print(\"The top 5 significant features with an inverse relationship to 'Execution': \")\n",
    "coefs['Feature_name'][coefs['Is Significant'] == True].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The coefficients with the maximum distance away from 0 are the more influential features. \n",
    "print(\"The top 5 significant features with a linear relationship to 'Execution': \")\n",
    "coefs['Feature_name'][coefs['Is Significant'] == True].tail(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic On Transformed PCA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the logistic regression performance on the transformed PCA data\n",
    "logreg = LogisticRegression(random_state = 0)\n",
    "logreg.fit(X_train_pca, train_labels)\n",
    "X_test_pca = pca.transform(test_data)\n",
    "print(\"The model accuracy for the test data is: \" + str(logreg.score(X_test_pca, test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### No PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = plot_confusion_matrix(lr_model, test_data, test_labels)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = lr_model.predict(test_data)\n",
    "print(classification_report(test_labels, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_roc_auc = roc_auc_score(test_labels, lr_model.predict_proba(test_data)[:,1])\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, lr_model.predict_proba(test_data)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % log_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = plot_confusion_matrix(logreg, X_test_pca, test_labels)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logreg = logreg.predict(X_test_pca)\n",
    "print(classification_report(test_labels, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_roc_auc = roc_auc_score(test_labels, logreg.predict_proba(X_test_pca)[:,1])\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, logreg.predict_proba(X_test_pca)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logreg_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude the results of our baseline model, we learned that despite the performance shown in the first ROC curve, there are a number of significant relationships within the data (see significance table above). Between the model that uses PCA and the one that does not, the model with PCA performs significantly better by 10 percentage points. **Most importantly, we found that gender related variables such as 'Sex', 'Midwifery', 'Dancing' and 'Singing' had statistically significant effects on whether the accused was executed. The model results show that the odds of a female accusee being executed were 1.6 times that of a male accusee, when controlling for all other variables.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we try to use more robust models that may find nonlinear patterns in the data. Although the nonlinear nature prevents us from interpreting the results and finding relationships as in the logistic regression, we hope to increase our model performance with additional freedoms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, attempting to see whether I can get a keras neural net working \n",
    "\n",
    "# Applying dimensionality reduction to our data\n",
    "X_train_nn = X_train_pca_df.iloc[:, 0:2]\n",
    "train_labels_nn = X_train_pca_df.iloc[:, 2]\n",
    "\n",
    "test_data_pca = pca.transform(test_data.fillna(0))\n",
    "test_data_nn = pd.DataFrame(test_data_pca)\n",
    "\n",
    "# Define the keras model \n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=2, activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the keras model \n",
    "model.compile(loss=keras.losses.BinaryCrossentropy(), \n",
    "              optimizer=\"adam\", \n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "# Fit the keras model \n",
    "model.fit(X_train_nn, train_labels_nn, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Evaluate the keras model \n",
    "_, binary_accuracy = model.evaluate(X_train_nn, train_labels_nn, verbose=0)\n",
    "print(f\"Training Binary Accuracy = {binary_accuracy}\")\n",
    "\n",
    "_, binary_accuracy= model.evaluate(test_data_nn, test_labels, verbose=0)     \n",
    "\n",
    "#######Could the accuracy be different because you need test_labels_nn? ^\n",
    "print(f\"Testing Binary Accuracy = {binary_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net: Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying dimensionality reduction to our data\n",
    "X_train_nn = X_train_pca_df.iloc[:, 0:2]\n",
    "train_labels_nn = X_train_pca_df.iloc[:, 2]\n",
    "\n",
    "test_data_pca = pca.transform(test_data.fillna(0))\n",
    "test_data_nn = pd.DataFrame(test_data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((X_train_nn, test_data_nn), axis=0)\n",
    "targets = np.concatenate((train_labels_nn, test_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(train_data, train_labels, test_data, test_labels, num_components):\n",
    "    \n",
    "    inputs = np.concatenate((train_data, test_data), axis=0)\n",
    "    targets = np.concatenate((train_labels, test_labels), axis=0)\n",
    "    \n",
    "    num_folds = 10\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "    acc_per_fold = []\n",
    "    loss_per_fold = []\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    fold_no = 1\n",
    "    for train, test in kfold.split(inputs, targets):\n",
    "\n",
    "        # Define the keras model \n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_dim=num_components, activation=\"relu\"))\n",
    "        model.add(Dense(8, activation=\"relu\"))\n",
    "        model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "        # Compile the keras model \n",
    "        model.compile(loss=keras.losses.BinaryCrossentropy(), \n",
    "                      optimizer=\"adam\", \n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        # Generate a print\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "        # Fit data to model\n",
    "        history = model.fit(inputs[train], \n",
    "                            targets[train], \n",
    "                            epochs=50, \n",
    "                            batch_size=10, \n",
    "                            verbose=0)\n",
    "\n",
    "        # Generate generalization metrics\n",
    "        scores = model.evaluate(inputs[test], targets[test], verbose=0)\n",
    "        print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "        acc_per_fold.append(scores[1] * 100)\n",
    "        loss_per_fold.append(scores[0])\n",
    "\n",
    "        # Increase fold number\n",
    "        fold_no = fold_no + 1\n",
    "        \n",
    "            # == Provide average scores ==\n",
    "    # == Provide average scores ==\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Score per fold')\n",
    "    for i in range(0, len(acc_per_fold)):\n",
    "        print('------------------------------------------------------------------------')\n",
    "        print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print('Average scores for all folds:')\n",
    "    print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
    "    print(f'> Loss: {np.mean(loss_per_fold)}')\n",
    "    print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2, 7):\n",
    "    \n",
    "    cv_train_data = pca_train_dataframes[i - 2]\n",
    "    cv_test_data = pca_test_dataframes[i - 2]\n",
    "    cv_train_labels = train_labels\n",
    "    cv_test_labels = test_labels\n",
    "    \n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Cross Validation Results for n={i} Principal Components\")\n",
    "    cross_validation(cv_train_data, cv_train_labels, cv_test_data, cv_test_labels, i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net: Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer NN GridSearch CV\n",
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "def create_model(learn_rate, \n",
    "                 activation,\n",
    "                 neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=2, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(), \n",
    "                  optimizer=\"Adam\", \n",
    "                  metrics=[keras.metrics.BinaryAccuracy()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(model=create_model, verbose=0,\n",
    "                        optimizer=\"Adam\", learn_rate=0.001, \n",
    "                        loss=keras.losses.BinaryCrossentropy(), epochs=10,\n",
    "                        batch_size=10, activation=\"sigmoid\", neurons=10)\n",
    "\n",
    "# GridSearchCV to test for several hyperparameters\n",
    "hyper_params = {\n",
    "    'batch_size': ([10, 20, 50]),\n",
    "    'epochs': ([10, 20, 50]),\n",
    "    'learn_rate': ([0.001, 0.01, 0.1]),\n",
    "    'activation': (['softmax', 'tanh', 'sigmoid']),\n",
    "    'neurons': ([10, 15, 20, 25, 30]) \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=hyper_params, n_jobs=-1, cv=3, verbose=3, error_score=\"raise\")\n",
    "grid_result = grid.fit(inputs, targets)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-layer NN GridSearchCV\n",
    "def create_model(learn_rate,\n",
    "                 activation,\n",
    "                 neurons):\n",
    "    model = Sequential()\n",
    "    # 1st layer \n",
    "    model.add(Dense(neurons, input_dim=2, activation=activation))\n",
    "    model.add(Dense(neurons - 4, activation=activation))\n",
    "    model.add(Dense(1, activation=activation))\n",
    "\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(), \n",
    "                  optimizer=\"Adam\", \n",
    "                  metrics=[keras.metrics.BinaryAccuracy()])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(model=create_model, verbose=False,\n",
    "                        optimizer=\"Adam\", learn_rate=0.1, \n",
    "                        loss=keras.losses.BinaryCrossentropy(), \n",
    "                        epochs=50, batch_size=10, \n",
    "                        activation=\"sigmoid\", neurons=10)\n",
    "\n",
    "# GridSearchCV to test for several hyperparameters\n",
    "hyper_params = {\n",
    "    'batch_size': ([10, 20]),\n",
    "    'epochs': ([10, 20]),\n",
    "    'learn_rate': ([0.001, 0.01, 0.1]),\n",
    "    'activation': ([\"softsign\", \"sigmoid\", \"tanh\"]),\n",
    "    'neurons': ([10, 15, 20, 25, 30]) \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=hyper_params, n_jobs=-1, cv=3, verbose=3, error_score=\"raise\")\n",
    "grid_result = grid.fit(inputs, targets)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=2, activation='tanh'))\n",
    "model.add(Dense(1, activation='tanh'))\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss=keras.losses.BinaryCrossentropy(), \n",
    "              optimizer=opt, \n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nn = X_train_pca_df.iloc[:, 0:2]\n",
    "train_labels_nn = X_train_pca_df.iloc[:, 2]\n",
    "\n",
    "test_data_pca = pca.transform(test_data.fillna(0))\n",
    "test_data_nn = pd.DataFrame(test_data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(X_train_nn, train_labels_nn, epochs=20, batch_size=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_data_nn, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model Accuracy {scores[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Net Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing model performance with the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_data_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = pd.DataFrame(metrics.confusion_matrix(test_labels, 1*(y_pred>=0.5)))\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, 1*(y_pred>=0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_roc_auc = roc_auc_score(test_labels, y_pred)\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, y_pred)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Neural Network (area = %0.2f)' % nn_roc_auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('Log_ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In conclusion, although in general cases neural networks perform better compared to the logistic regression model, the logistic regression outperforms in this case. In addition, explainability is very important in this case, so we would choose the logistic regression.** Today, it is no longer necessary to predict witch trial executions, but it is necessary to be able to interpret the causes of these historical effects and to be able to make connections to gender issues that continue to exist today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contributions\n",
    "\n",
    "**Elda Pere**: Worked on collecting the data, researching 15th-18th century witchcraft in Scotland and how it relates to gender, balancing the target variable with SMOTE, determining significance and interpreting results of logistic regression, evaluating and visualizing the performance of the models that are run, and working on the presentation. \n",
    "\n",
    "**Kevin Gu**: Worked on data exploration, cleaning/preprocessing, feature engineering, and PCA dimensionality reduction\n",
    "\n",
    "**Alejandro Pelcastre**: Worked on initial data exploration, data cleaning, probability distribution function (to fill na), Decision Tree (scraped because didn't add much), created baseline Logistic Regression model plus L1 regularization and on transformed data\n",
    "\n",
    "**Carlos Calderon**: Assisted in data collection. Worked on cross validation and grid search cross validation for Neural Net construction. Assisted in the exploratory data analysis process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
